
import argparse
import torch
import json
import os
from sentence_transformers import SentenceTransformer
from transformers import CLIPProcessor, CLIPModel, AutoTokenizer, AutoModel
from PIL import Image
from tqdm import tqdm
import torch.nn.functional as F
from concurrent.futures import ProcessPoolExecutor
from config import *

parser = argparse.ArgumentParser()
parser.add_argument("--source_path", type=str, help="ì´ë¯¸ì§€ëŠ” í´ë” ê²½ë¡œ(ì‰¼í‘œë¡œ ë‹¤ì¤‘ ì§€ì • ê°€ëŠ¥), í…ìŠ¤íŠ¸ëŠ” íŒŒì¼ ê²½ë¡œ", default=None)
parser.add_argument("--type", type=str, choices=["image", "korean"], default=None)
parser.add_argument("--model", type=str, default=None)
parser.add_argument("--batch_size", type=int, default=16, help="ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸ê°’: 16)")
args = parser.parse_args()

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

def mean_pooling(model_output, attention_mask):
    token_embeddings = model_output[0] #First element of model_output contains all token embeddings
    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)

def load_image(fpath):
    try:
        image = Image.open(fpath).convert("RGB")
        return image
    except Exception as e:
        print(f"âŒ ì´ë¯¸ì§€ ì—´ê¸° ì‹¤íŒ¨: {fpath}, ì˜¤ë¥˜: {e}")
        return None

@torch.no_grad()
# TODO: refactor get_vision_feature
def get_vision_feature():
    print(f"ğŸ” ëª¨ë¸ ë¡œë”© ì¤‘...")
    model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32").to(device)
    processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

    image_features = []

    if isinstance(args.source_path, str):
        args.source_path = [p.strip() for p in args.source_path.split(",")]

    all_files = []
    for path in args.source_path:
        print(f"ğŸ“‚ íƒìƒ‰ ì¤‘: {path}")
        if not os.path.exists(path):
            print(f"âŒ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {path}")
            continue
        
        with os.scandir(path) as it:
            all_files.extend([entry.path for entry in it if entry.is_file() and 'train' in entry.name])

    print(f"âœ… ë¡œë”©ëœ íŒŒì¼ ê°œìˆ˜: {len(all_files)}")

    if len(all_files) == 0:
        print("âš ï¸ ë¡œë”©ëœ ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œì™€ íŒŒì¼ í˜•ì‹ì„ í™•ì¸í•˜ì„¸ìš”.")
        return None

    print(f"ğŸš€ ì´ë¯¸ì§€ ë¡œë”© ì¤‘ (ë©€í‹°í”„ë¡œì„¸ì‹±)...")
    all_images = []
    with ProcessPoolExecutor() as executor:
        for image in tqdm(executor.map(load_image, all_files), total=len(all_files), desc="ğŸ“¥ ì´ë¯¸ì§€ ë¡œë”© ì¤‘"):
            if image is not None:
                all_images.append(image)

    print(f"âœ… ë¡œë”©ëœ ì´ë¯¸ì§€ ê°œìˆ˜: {len(all_images)}")

    
    for i in tqdm(range(0, len(all_images), args.batch_size), desc="ğŸ“Š ì´ë¯¸ì§€ ì„ë² ë”© ì¤‘ (GPU ì‚¬ìš© ì¤‘)"):
        batch = all_images[i:i + args.batch_size]
        inputs = processor(images=batch, return_tensors="pt", padding=True).to(device)
        features = model.get_image_features(**inputs)
        features = F.normalize(features, dim=-1)
        image_features.append(features)
    
    image_embeddings = torch.cat(image_features, dim=0)
    print(f'ğŸ” ì„ë² ë”© ì™„ë£Œ - Original Image Count: {len(all_images)}, Embedding Size: {image_embeddings.size()}')
    return image_embeddings

@torch.no_grad()
def get_korean_feature():
    print(f"ğŸ” ëª¨ë¸ ë¡œë”© ì¤‘...")
    if args.model == "e5":
        model = SentenceTransformer(E5_MODEL_PATH).to(device)
    elif args.model == "minilm":
        model = SentenceTransformer(MINILM_MODEL_PATH).to(device)
    
    print(f"ğŸ“‚ JSON íŒŒì¼ íƒìƒ‰ ì¤‘: {args.source_path}")
    
    all_captions = []


    with open(args.source_path, "r", encoding="utf-8") as f:
        data = json.load(f)   # <== ì—¬ê¸°ì„œ ì „ì²´ë¥¼ ë¡œë”©í•©ë‹ˆë‹¤.

    # ê° í•­ëª©ì—ì„œ caption_koë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    for obj in data:
        if "caption_ko" in obj and isinstance(obj["caption_ko"], list):
            valid_captions = [caption for caption in obj['caption_ko'] if isinstance(caption, str)]
            all_captions.extend(valid_captions)

    print(all_captions[:10])  # ë¡œë”©ëœ ìº¡ì…˜ì˜ ì¼ë¶€ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.

    if args.model == "e5":
        all_captions = ["query: " + caption for caption in all_captions]

    print(f"âœ… ë¡œë”©ëœ ìº¡ì…˜ ê°œìˆ˜: {len(all_captions)}")
    if len(all_captions) == 0:
        print("âš ï¸ ë¡œë”©ëœ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. JSON êµ¬ì¡°ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        return None

    # ======= ì„ë² ë”© ì¶”ì¶œ =======
    print(f"ğŸš€ í…ìŠ¤íŠ¸ ì„ë² ë”© ì¶”ì¶œ ì¤‘...")
    all_embeddings = []

    for i in tqdm(range(0, len(all_captions), args.batch_size), desc="ğŸ“Š í…ìŠ¤íŠ¸ ì„ë² ë”© ì¤‘"):
        batch = all_captions[i:i + args.batch_size]
        outputs = model.encode(batch, device=device, convert_to_tensor=True, normalize_embeddings=True)
        all_embeddings.append(outputs)

    all_embeddings = torch.cat(all_embeddings, dim=0)
    print(f'ğŸ” ì„ë² ë”© ì™„ë£Œ - Original Text Count: {len(all_captions)}, Embedding Size: {all_embeddings.size()}, Embedding Length: {torch.norm(all_embeddings[0], p=2)}')
    return all_embeddings
# ======= Main Execution =======
if args.type == "image":
    image_embedding = get_vision_feature()
    if image_embedding is not None:
        torch.save(image_embedding, "image_embedding.pt")
        print("âœ… ì´ë¯¸ì§€ ì„ë² ë”© ì €ì¥ ì™„ë£Œ: image_embedding.pt")
elif args.type == "korean":
    text_embedding = get_korean_feature()
    if text_embedding is not None:
        torch.save(text_embedding, "text_embedding.pt")
        print("âœ… í…ìŠ¤íŠ¸ ì„ë² ë”© ì €ì¥ ì™„ë£Œ: text_embedding.pt")
else:
    print("âŒ ì˜ëª»ëœ íƒ€ì…ì…ë‹ˆë‹¤. --type ì¸ìëŠ” 'image' ë˜ëŠ” 'korean'ì´ì–´ì•¼ í•©ë‹ˆë‹¤.")
